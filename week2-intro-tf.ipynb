{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to TFLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Siraj:\n",
    "- how to build a neural network for sentiment analysis using a library called TFLearn. \n",
    "\n",
    "----\n",
    "> In this lesson\n",
    "- started with the library and provide some more background so you can get the most out of his video.\n",
    "\n",
    "----\n",
    "> TFLearn\n",
    "- does a lot of the heavy lifting \n",
    "    - initializing weights\n",
    "    - running the forward pass\n",
    "    - taking care of backpropagation\n",
    "\n",
    "----    \n",
    "> First learning more about activation functions and a new cost function\n",
    "- Activation functions: Rectified linear units (ReLUs) and Softmax\n",
    "- Classification cost function: Categorical Cross Entropy\n",
    "\n",
    "----\n",
    "> Sentiment Analysis with TFLearn\n",
    "\n",
    "----\n",
    "> Handwritten digit recognition with TFLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **purpose of activation function?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why ReLU over sigmoid function as activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "> sigmoid function: \n",
    "- used as activation function on our hidden units and on the output unit. \n",
    "\n",
    "----\n",
    "> **Why sigmoids have fallen out of favor as activations on hidden units**\n",
    "- the **derivative of the sigmoid** maxes out at 0.25 (see below). \n",
    "- error of hidden layers is calculated using output error or derivative of E\n",
    "- meaning the errors going back into the network will be **shrunk by at least a quarter at every layer**\n",
    "- For **layers close to the input layer**, the weight updates will be tiny if you have a lot of layers and those weights will **take a really long time to train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/February/5893d15c_sigmoids/sigmoids.png\" width=\"500\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width=500, height=300, url='https://d17h27t6h515a5.cloudfront.net/topher/2017/February/5893d15c_sigmoids/sigmoids.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Rectified Linear Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **rectified linear units (ReLUs)**\n",
    "- Instead of sigmoids, **most recent deep learning networks use it for the hidden layers**. \n",
    "- A rectified linear unit has output 0 if the input is less than 0, and raw output otherwise. $$f(x)=\\max(x,0)$$\n",
    "- see image below for the function behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58915ae8_relu/relu.png\" width=\"300\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width=300, height=100, url='https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58915ae8_relu/relu.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ReLU activations** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the simplest non-linear activation function\n",
    "- When the **input is positive, the derivative is 1**, so **no vanishing effect** on backpropagated errors from sigmoids \n",
    "- ReLUs result in much **faster** training for large networks [based on research](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf). \n",
    "- **No need implementation** of relu ourselves in most frameworks like TensorFlow and TFLearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks\n",
    "- not clear what it means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My summary of what ReLU drawbacks is\n",
    "- check [forum response on this](https://discussions.udacity.com/t/how-to-understand-relus-drawbacks-properly/217556?u=pystrategyexplorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLU drawbacks is when learning_rate is too large, can cause many neuron death while model is still training**\n",
    "\n",
    "The neuron death is achieved in both forward and backward pass together. \n",
    "\n",
    "----\n",
    "> **Forward pass: **\n",
    "- If learning_rate was set very large, it leads to overshoot;\n",
    "- overshoot or divergence leads to large derivative or gradient;\n",
    "- large gradient leads to big step change on weights; \n",
    "- such big change on weights that it causes all inputs flowing toward ReLU function are turned negative; \n",
    "- therefore, negative inputs to ReLU function by formula will always return 0;\n",
    "- ReLU neural units don't fire, don't activate, are dead\n",
    "\n",
    "----\n",
    "> **Backward pass: **\n",
    "- to update hidden weights, we need hidden_delta;\n",
    "- to calc hidden_delta, we need hidden_error * derivative of ReLU(hidden_inputs)\n",
    "- given hidden_inputs are negative (caused ReLU dead in forward pass), derivative of ReLU(hidden_inputs) can only return zero\n",
    "- therefore, weight step for updating is also zero. \n",
    "- So, there is no update of hidden layer weights, there is no fire from hidden_ReLU neurons\n",
    "- there is no forward and backward update at all and forever, this is neurons are (brain) dead but training (body) is still running\n",
    "\n",
    "----\n",
    "> Therefore, solution is to reduce learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### What lesson and google say about ReLU drawbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> drawbacks: \n",
    "- It's possible that a large gradient can set the weights such\n",
    "- that a ReLU unit will always be 0 (meaning, relu unit is dead)\n",
    "- a lot of computation will be wasted in training (meaning, compute a lot but return just 0s, so it is a waste)\n",
    "\n",
    "----\n",
    "> From Andrej Karpathy's CS231n course:\n",
    "- Unfortunately, ReLU units can be fragile during training and can “die”. \n",
    "- a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again\n",
    "- If this happens, then **the gradient flowing through the unit will forever be zero** from that point on. \n",
    "- the ReLU units can irreversibly die during training since they can get knocked off the data manifold. \n",
    "- you may find that as much as 40% of your network can be “dead” (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. \n",
    "\n",
    "----\n",
    "googled it and here is [something seemingly make some sense to me](https://www.reddit.com/r/MachineLearning/comments/3ij6nz/confused_about_why_relus_show_benefit_in_deep/)\n",
    "> In back-propagation, you need to know the derivative of the non-linear function [my comment: which is relu, I think] to figure out how to change the weights before that function. If that derivative is zero [my comment: meaning gradient is zero], you've basically destroyed information.\n",
    "\n",
    "\n",
    "----\n",
    "> If you stare at this for a while you’ll see that if a neuron gets clamped to zero in the forward pass (i.e. z=0, it doesn’t “fire”), then its weights will get zero gradient. This can lead to what is called the “dead ReLU” problem, where if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron’s weights ever get knocked off with a large update during training into this regime, then this neuron will remain permanently dead. It’s like permanent, irrecoverable brain damage. Sometimes you can forward the entire training set through a trained network and find that a large fraction (e.g. 40%) of your neurons were zero the entire time.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*g0yxlK8kEBw8uA1f82XQdA.png)\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "> solution: \n",
    "- With a proper setting of the learning rate this is less frequently an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> sigmoid function\n",
    "- works for neural networks used for regression (bike riders) and binary classification (graduate school admissions)\n",
    "- **Not for multi-class classification problem** \n",
    "\n",
    "----\n",
    "> softmax function\n",
    "- is **for multi-class classification problem** \n",
    "- The softmax function squashes the outputs of each unit to be between 0 and 1, just like a sigmoid. \n",
    "- It also divides each output such that the total sum of the outputs is equal to 1. \n",
    "- The output of the softmax function is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true.\n",
    "\n",
    "----\n",
    "> if you have three inputs to a softmax function, say from a network with three output units, it'd look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58950908_softmax-input-output/softmax-input-output.png\" width=\"300\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width=300, height=100, url='https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58950908_softmax-input-output/softmax-input-output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "> Formula of Softmax\n",
    "- Mathematically the softmax function is shown below, where z is a vector of the inputs to the output layer (if you have 10 output units, then there are 10 elements in z). And again, j indexes the output units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58938e9e_softmax-math/softmax-math.png\" width=\"300\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width=300, height=100, url='https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58938e9e_softmax-math/softmax-math.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> simple behaviour but different math to visualize\n",
    "- This admittedly looks daunting to understand, but it's actually quite simple and it's fine if you don't get the math. Just remember that the outputs are squashed and they sum to one.\n",
    "- The network would have ten output units, one for each digit 0 to 9. \n",
    "- if you fed it an image of a number 4 (see below), the output unit corresponding to the digit 4 would be activated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "> the output of the softmax function might look like\n",
    "- Building a network like this requires 10 output units, one for each digit. \n",
    "- if the input is an image of the digit 4, the output unit corresponding to 4 would be activated, and so on for the rest of the units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58925c7f_softmax/softmax.png\" width=\"300\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width=300, height=100, url='https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58925c7f_softmax/softmax.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- digit 4, 7, 9 have higher probabilities, 4 has the most\n",
    "- The softmax can be used for any number of classes, even thousands of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to plot this function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lesson, this formula above is admitted as \"daunting\", but I think our instructors have done a great job in simplify it. Because when I checked softmax function in wikipedia, that is truly daunting. \n",
    "\n",
    "Personally I don't think I need to have a deep understanding of the math behind the function. As long as I can code the function and plot it, I am satisfied. \n",
    "\n",
    "With my shallow python skill, I somehow still believe I should be able to plot it. \n",
    "\n",
    "Below is my attemp, could you verify it for me? Thanks a lot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: coding the function\n",
    "- I don't understand why there must be two variables in the formula $j, k$, my reading of the formula is that $j, k$ are the same thing. I am probably wrong, please correct me. \n",
    "- After I regard $j, k$ as the same thing, I only keep $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11c5537b8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXsQkSQ2y3CWpQZa7JDXIcpekBv0/0lzZswE+TsYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c5437f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# My reading of the formula above is j and k are the same variable, so I use j to replace k \n",
    "# in the above formula\n",
    "\n",
    "import pylab\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "# input z vector\n",
    "# z = [5,2,9] # thinking of it as 3 image inputs \n",
    "# z = [2,2,2,2,2,2,2,2,2,2] # 10 image inputs\n",
    "z = [1,2,3,4,5,6,7,8,9,10]\n",
    "# z = np.random.rand(10)*3 # 10 linearly spaced numbers\n",
    "\n",
    "# set empty list for y\n",
    "y = []\n",
    "\n",
    "# calculate y\n",
    "K = len(z)\n",
    "bottom = 0\n",
    "for j in range(K):\n",
    "    top = np.exp(z[j])\n",
    "    bottom = top + bottom\n",
    "    y.append(top/bottom)\n",
    "\n",
    "    \n",
    "data = pd.DataFrame({'x':z, 'y':y})\n",
    "# pylab.plot(z,y) # sin(x)/x\n",
    "data.y.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Cross-Entropy\n",
    "- cost function for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "> **SSE as cost function: **\n",
    "- using the sum of squared errors as the cost function in our networks,\n",
    "- when expecting only singular (scalar) output values.\n",
    "\n",
    "----\n",
    "> **multi-class classification problems: **\n",
    "- using softmax\n",
    "- output is a vector of probability values from the output units\n",
    "- **data labels** can be expressed as a vector using **one-hot encoding**\n",
    "\n",
    "----\n",
    "> **How to express predictions and labels**\n",
    "- you have a vector the length of the number of classes\n",
    "- the label element is marked with a 1 while the other labels are set to 0. \n",
    "- our label vector for the image of the number 4 would be: $$ y=[0,0,0,0,1,0,0,0,0,0]$$\n",
    "- our output prediction vector could be something like $$ \\hat{y}=[0.047,0.048,0.061,0.07,0.330,0.062,0.001,0.213,0.013,0.150]$$\n",
    "\n",
    "----\n",
    "> **We want our error to be proportional to how far apart these vectors are**\n",
    "- To calculate this distance, use the cross entropy\n",
    "- our goal: to make our prediction vectors as close as possible to the label vectors by minimizing the cross entropy. \n",
    "\n",
    "----\n",
    "> **The cross entropy calculation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/February/5893b106_cross-entropy-diagram/cross-entropy-diagram.png\" width=\"300\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width=300, height=100, url='https://d17h27t6h515a5.cloudfront.net/topher/2017/February/5893b106_cross-entropy-diagram/cross-entropy-diagram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "> **Calculate cross entropy**: \n",
    "- the sum of the label elements times the (natural) log of the prediction probabilities\n",
    "- this formula is not symmetric\n",
    "- Flipping the vectors is a bad idea because the label vector has a lot of zeros and taking the log of zero will cause an error\n",
    "\n",
    "----\n",
    "> **What's cool about using one-hot encoding for the label vector**\n",
    "- $y_j$ is 0 except for the one true class \n",
    "- all terms in that sum except for where $y_j=1$ are zero \n",
    "- the cross entropy is simply $D=−\\log{\\hat{y}}$ for the true label\n",
    "- if your input image is of the digit 4 and it's labeled 4, then only the output of the unit corresponding to 4 matters in the cross entropy cost.\n",
    "\n",
    "----\n",
    "> **summary:**\n",
    "- cross entropy is multi-class problem cost function\n",
    "- calc cross entropy using $D(\\hat{y}, y) = - \\sum_j y_j\\log{\\hat{y_j}}$\n",
    "- this forumla can be simplified: $D(\\hat{y}, y) = E = -\\log{\\hat{y_j}}$\n",
    "- whereas $ SSE = E = \\frac{1}{2}\\sum(y-\\hat{y})^2 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUIZ QUESTION\n",
    "\n",
    "If your label vector is [0, 0, 0, 1, 0] and the predicted probabilities are [0.27, 0.11, 0.33, 0.10, 0.19], what is the cross entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025850929940455"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "E = - np.log(0.1)\n",
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with TFLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What can TFLearn do for us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- initializing weights\n",
    "- running the forward pass\n",
    "- performing backpropagation to update the weights\n",
    "- You end up just defining the architecture of the network (number and type of layers, number of units, etc.) and how it is trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/bFxfodsBbzU?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/bFxfodsBbzU?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
